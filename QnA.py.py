# -*- coding: utf-8 -*-
"""Copy of QuestionAnswering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGucuDy0QmKn4mBhaUrCJPw-M6HGm_cL
"""

!pip install torch

!pip install transformers

#importing libaries
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

QA_input = [{'question': 'Why is conversion important?',
             'context': 'The option to convert models between FARM and transformers gives freedom to the users and let people easily switch between frameworks.'},
            {'question': 'How many programming languages does BLOOM support?',
             'context': 'BLOOM has 176 billion parameters and can ganerate text in 46 natural languages and 13 programming languages.'}]

#select from thr hub of pretrained models: https://huggingface.co/models
model_name = 'deepset/roberta-base-squad2'

#method 1
model1= AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

inputs0 = tokenizer(QA_input[0]['question'], QA_input[0]['context'], return_tensors="pt")
output0 = model1(**inputs0)

inputs1 = tokenizer(QA_input[1]['question'], QA_input[1]['context'], return_tensors="pt")
output1 = model1(**inputs1)

output0

answer_start_idx = torch.argmax(output0.start_logits)
answer_end_idx = torch.argmax(output0.end_logits)

answer_tokens = inputs0.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)
print("ques: {}\nanswer: {}".format(QA_input[0]['question'], answer))

answer_start_idx = torch.argmax(output1.start_logits)
answer_end_idx = torch.argmax(output1.end_logits)

answer_tokens = inputs1.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)
print("ques: {}\nanswer: {}".format(QA_input[1]['question'], answer))

#method 2: simple and direct.....not all models are compatible for pipeline
qa = pipeline('question-answering', model=model_name, tokenizer=model_name)

output_0 = qa(QA_input[0]['question'], QA_input[0]['context'])
print(output_0)

output_1 = qa(QA_input[1]['question'], QA_input[1]['context'])
print(output_1)